{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall numpy==1.23.5 pandas==1.5.3 --no-cache-dir --quiet"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-ml 0.6.1 requires enum34, which is not installed.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.0 requires pillow>=10.1, but you have pillow 9.2.0 which is incompatible.\nresponsibleai 0.36.0 requires networkx<=2.5, but you have networkx 3.4 which is incompatible.\nopencensus-ext-azure 1.1.14 requires psutil>=5.6.3, but you have psutil 5.2.2 which is incompatible.\ndask-sql 2024.5.0 requires dask[dataframe]>=2024.4.1, but you have dask 2023.2.0 which is incompatible.\ndask-sql 2024.5.0 requires distributed>=2024.4.1, but you have distributed 2023.2.0 which is incompatible.\ndask-expr 1.1.10 requires dask==2024.8.0, but you have dask 2023.2.0 which is incompatible.\ndask-expr 1.1.10 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\nazureml-training-tabular 1.60.0 requires scipy<1.11.0,>=1.0.0, but you have scipy 1.15.3 which is incompatible.\nazureml-train-automl-runtime 1.60.0 requires scipy<=1.11.0,>=1.0.0, but you have scipy 1.15.3 which is incompatible.\nazureml-mlflow 1.60.0 requires azure-storage-blob<=12.19.0,>=12.5.0, but you have azure-storage-blob 12.25.1 which is incompatible.\nazureml-automl-runtime 1.60.0 requires scipy<=1.11.0,>=1.0.0, but you have scipy 1.15.3 which is incompatible.\nazureml-automl-dnn-nlp 1.60.0 requires torch==2.2.2, but you have torch 2.6.0 which is incompatible.\nazure-cli 2.71.0 requires azure-keyvault-keys==4.9.0b3, but you have azure-keyvault-keys 4.8.0 which is incompatible.\nazure-cli 2.71.0 requires azure-mgmt-keyvault==11.0.0, but you have azure-mgmt-keyvault 10.3.1 which is incompatible.\nazure-cli 2.71.0 requires azure-mgmt-storage==22.1.0, but you have azure-mgmt-storage 22.0.0 which is incompatible.\nazure-cli 2.71.0 requires websocket-client~=1.3.1, but you have websocket-client 1.8.0 which is incompatible.\nazure-cli-core 2.71.0 requires psutil>=5.9; sys_platform != \"cygwin\", but you have psutil 5.2.2 which is incompatible.\nadlfs 2024.12.0 requires azure-datalake-store<0.1,>=0.0.53, but you have azure-datalake-store 1.0.0a0 which is incompatible.\nadlfs 2024.12.0 requires fsspec>=2023.12.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1746780902141
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests xgboost scikit-learn matplotlib --quiet\n",
        "!pip install seaborn --quiet"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/anaconda/envs/azureml_py38/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n\u001b[0m"
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install xgboost --quiet"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1746780600609
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install seaborn --quiet"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1746780601736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install shap --quiet"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1746780602979
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def fetch_polygon_data_range_5min(symbol, start_date, end_date, api_key):\n",
        "    url = (\n",
        "        f\"https://api.polygon.io/v2/aggs/ticker/{symbol}/range/5/minute/\"\n",
        "        f\"{start_date}/{end_date}?adjusted=true&sort=asc&limit=50000&apiKey={api_key}\"\n",
        "    )\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"results\" in data:\n",
        "        df = pd.DataFrame(data[\"results\"])\n",
        "        df['t'] = pd.to_datetime(df['t'], unit='ms')\n",
        "        df.rename(columns={\n",
        "            't': 'timestamp', 'o': 'open', 'h': 'high',\n",
        "            'l': 'low', 'c': 'close', 'v': 'volume', 'n': 'trades'\n",
        "        }, inplace=True)\n",
        "        return df\n",
        "    else:\n",
        "        print(\"Error:\", data.get(\"error\", \"Unknown error\"))\n",
        "        return pd.DataFrame()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_polygon_data_range_5min\u001b[39m(symbol, start_date, end_date, api_key):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1746780603101
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "symbol = \"SPY\"\n",
        "API_KEY = \"AXOcwAzJT1ijsBXWK_9svkDpMpviJ94q\"  # Make sure Block 1 is already run\n",
        "\n",
        "# Start from May 2020\n",
        "start_year = 2020\n",
        "start_month = 5\n",
        "\n",
        "# Today's date\n",
        "today = datetime.today()\n",
        "end_year = today.year\n",
        "end_month = today.month - 1  # Don't fetch current month (not complete)\n",
        "\n",
        "for year in range(start_year, end_year + 1):\n",
        "    for month in range(1, 13):\n",
        "        if year == start_year and month < start_month:\n",
        "            continue\n",
        "        if year == end_year and month > end_month:\n",
        "            break\n",
        "\n",
        "        start_date = f\"{year}-{month:02d}-01\"\n",
        "        end_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\") + timedelta(days=32)\n",
        "        end_date = end_date_dt.replace(day=1).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        print(f\"Fetching {symbol} 5-min data: {start_date} to {end_date}...\")\n",
        "        df = fetch_polygon_data_range_5min(symbol, start_date, end_date, API_KEY)\n",
        "\n",
        "        if not df.empty:\n",
        "            filename = f\"{symbol}_5min_{year}_{month:02d}.csv\"\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"Saved to {filename} ({len(df)} rows)\\n\")\n",
        "        else:\n",
        "            print(f\"No data for {start_date} to {end_date}\")\n",
        "\n",
        "        time.sleep(2)  # Respect Polygon rate limit"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605355
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all monthly files\n",
        "files = sorted([f for f in os.listdir() if f.startswith(\"SPY_5min_\") and f.endswith(\".csv\")])\n",
        "\n",
        "# Read and concatenate all DataFrames\n",
        "all_data = []\n",
        "for file in files:\n",
        "    df = pd.read_csv(file, parse_dates=[\"timestamp\"])\n",
        "    all_data.append(df)\n",
        "\n",
        "full_df = pd.concat(all_data).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "# Save combined file\n",
        "full_df.to_csv(\"SPY_5min_full.csv\", index=False)\n",
        "\n",
        "# Preview the first few rows\n",
        "full_df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605378
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "\n",
        "# Ensure timestamp is datetime\n",
        "full_df[\"timestamp\"] = pd.to_datetime(full_df[\"timestamp\"])\n",
        "\n",
        "# Date breakdown\n",
        "full_df[\"date\"] = full_df[\"timestamp\"].dt.date\n",
        "full_df[\"time\"] = full_df[\"timestamp\"].dt.time\n",
        "full_df[\"day_of_week\"] = full_df[\"timestamp\"].dt.dayofweek          # 0 = Monday\n",
        "full_df[\"is_monday\"] = (full_df[\"day_of_week\"] == 0).astype(int)\n",
        "full_df[\"is_friday\"] = (full_df[\"day_of_week\"] == 4).astype(int)\n",
        "\n",
        "# Month & quarter structure\n",
        "full_df[\"month\"] = full_df[\"timestamp\"].dt.month\n",
        "full_df[\"year\"] = full_df[\"timestamp\"].dt.year\n",
        "full_df[\"quarter\"] = full_df[\"timestamp\"].dt.quarter\n",
        "full_df[\"is_month_start\"] = full_df[\"timestamp\"].dt.is_month_start.astype(int)\n",
        "full_df[\"is_month_end\"] = full_df[\"timestamp\"].dt.is_month_end.astype(int)\n",
        "full_df[\"is_quarter_start\"] = full_df[\"timestamp\"].dt.is_quarter_start.astype(int)\n",
        "full_df[\"is_quarter_end\"] = full_df[\"timestamp\"].dt.is_quarter_end.astype(int)\n",
        "\n",
        "# Season (Winter=1, Spring=2, Summer=3, Fall=4)\n",
        "full_df[\"season\"] = ((full_df[\"month\"] % 12 + 3) // 3)\n",
        "\n",
        "# Time-specific\n",
        "full_df[\"hour\"] = full_df[\"timestamp\"].dt.hour\n",
        "full_df[\"minute\"] = full_df[\"timestamp\"].dt.minute\n",
        "\n",
        "# Is first trading day of month\n",
        "full_df[\"prev_date\"] = full_df[\"date\"].shift(1)\n",
        "full_df[\"is_first_trading_day\"] = (full_df[\"date\"] != full_df[\"prev_date\"]).astype(int)\n",
        "full_df.loc[full_df[\"timestamp\"].dt.day != 1, \"is_first_trading_day\"] = 0\n",
        "\n",
        "# U.S. holiday logic\n",
        "cal = USFederalHolidayCalendar()\n",
        "holidays = cal.holidays(start=full_df[\"timestamp\"].min(), end=full_df[\"timestamp\"].max())\n",
        "full_df[\"is_after_holiday\"] = full_df[\"timestamp\"].dt.normalize().isin(holidays + pd.Timedelta(days=1)).astype(int)\n",
        "\n",
        "# Drop helper column\n",
        "full_df.drop(columns=[\"prev_date\"], inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only 9:30 and 9:35 bars\n",
        "target_df = full_df[full_df[[\"hour\", \"minute\"]].apply(tuple, axis=1).isin([(9, 30), (9, 35)])].copy()\n",
        "\n",
        "# Drop exact duplicate timestamps (if any)\n",
        "target_df = target_df.drop_duplicates(subset=[\"date\", \"hour\", \"minute\"])\n",
        "\n",
        "# Pivot to get one row per day with close_930 and close_935\n",
        "pivoted = target_df.pivot(index=\"date\", columns=[\"hour\", \"minute\"], values=\"close\")\n",
        "pivoted.columns = [\"close_930\", \"close_935\"]\n",
        "pivoted.reset_index(inplace=True)\n",
        "\n",
        "# Extract calendar features from the 9:30 bar rows\n",
        "calendar_cols = [\n",
        "    \"date\", \"day_of_week\", \"is_monday\", \"is_friday\",\n",
        "    \"month\", \"year\", \"quarter\", \"is_month_start\", \"is_month_end\",\n",
        "    \"is_quarter_start\", \"is_quarter_end\", \"season\",\n",
        "    \"is_first_trading_day\", \"is_after_holiday\"\n",
        "]\n",
        "calendar_features = full_df[(full_df[\"hour\"] == 9) & (full_df[\"minute\"] == 30)][calendar_cols]\n",
        "\n",
        "# Merge price and calendar features\n",
        "model_df = pd.merge(pivoted, calendar_features, on=\"date\", how=\"inner\")\n",
        "\n",
        "# Create binary target: 1 if price increased from 9:30 to 9:35\n",
        "model_df[\"went_up\"] = (model_df[\"close_935\"] > model_df[\"close_930\"]).astype(int)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 11: Train/Test Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = model_df.drop(columns=[\"date\", \"close_930\", \"close_935\", \"went_up\"])\n",
        "y = model_df[\"went_up\"]\n",
        "\n",
        "# 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Preview shapes\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605442
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Block 12: Train XGBoost classifier\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605454
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predict again (just in case)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605464
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Block 13: Plot feature importance\n",
        "importances = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, importances)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"XGBoost Feature Importance\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605474
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 13.1: Print feature importance scores\n",
        "importances = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Pair and sort by importance\n",
        "importance_list = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print nicely\n",
        "for name, score in importance_list:\n",
        "    print(f\"{name:25} {score:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605486
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Features used by the model:\")\n",
        "print(X.columns.tolist())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605497
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild model_df and model_df_clean\n",
        "\n",
        "# Create target\n",
        "model_df = pd.merge(pivoted, calendar_features, on=\"date\", how=\"inner\")\n",
        "model_df[\"went_up\"] = (model_df[\"close_935\"] > model_df[\"close_930\"]).astype(int)\n",
        "\n",
        "# Drop any NaNs and make a clean copy\n",
        "model_df_clean = model_df.dropna().copy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605506
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calendar_cols_only = [\n",
        "    'day_of_week', 'is_monday', 'is_friday', 'month', 'year',\n",
        "    'quarter', 'is_month_start', 'is_month_end', 'is_quarter_start',\n",
        "    'is_quarter_end', 'season', 'is_first_trading_day', 'is_after_holiday'\n",
        "]\n",
        "\n",
        "X = model_df_clean[calendar_cols_only]\n",
        "y = model_df_clean[\"went_up\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Calendar-Only Accuracy: {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605517
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 16: Add prev_day_return safely (confirmed working)\n",
        "\n",
        "# Step 1: Get 16:00 bars from full_df\n",
        "df_1600 = full_df[(full_df[\"hour\"] == 16) & (full_df[\"minute\"] == 0)].copy()\n",
        "df_1600[\"prev_day\"] = df_1600[\"timestamp\"].dt.normalize() + pd.Timedelta(days=1)\n",
        "df_1600 = df_1600.rename(columns={\"open\": \"prev_day_open\", \"close\": \"prev_day_close\"})\n",
        "df_1600 = df_1600[[\"prev_day\", \"prev_day_open\", \"prev_day_close\"]]\n",
        "\n",
        "# Step 2: Prepare model_df_clean with matching datetime format\n",
        "model_df_clean = model_df_clean.copy()\n",
        "model_df_clean[\"date\"] = pd.to_datetime(model_df_clean[\"date\"]).dt.normalize()\n",
        "\n",
        "# Step 3: MERGE on date (this is the actual merge step)\n",
        "model_df_fixed = pd.merge(\n",
        "    model_df_clean,\n",
        "    df_1600,\n",
        "    left_on=\"date\",\n",
        "    right_on=\"prev_day\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Step 4: Use correct column names directly (no renaming)\n",
        "# You likely have _x and _y suffixes after the merge\n",
        "print(\"Columns after merge:\", model_df_fixed.columns.tolist())\n",
        "\n",
        "# Step 5: Calculate return using the actual column names (adjust if needed)\n",
        "model_df_fixed[\"prev_day_return\"] = (\n",
        "    (model_df_fixed[\"prev_day_close\"] - model_df_fixed[\"prev_day_open\"]) / model_df_fixed[\"prev_day_open\"]\n",
        ")\n",
        "\n",
        "# Step 6: Drop rows with missing values\n",
        "model_df_fixed = model_df_fixed.dropna(subset=[\"prev_day_return\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605531
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = calendar_cols_only + [\"prev_day_return\"]\n",
        "X = model_df_fixed[feature_cols]\n",
        "y = model_df_fixed[\"went_up\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with prev_day_return: {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605549
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605562
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Print the confusion matrix as raw numbers\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605573
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set cutoff date for out-of-sample testing\n",
        "cutoff_date = pd.to_datetime(\"2024-11-01\")\n",
        "\n",
        "# Use model_df_fixed which includes prev_day_return\n",
        "train_df = model_df_fixed[model_df_fixed[\"date\"] < cutoff_date]\n",
        "test_df  = model_df_fixed[model_df_fixed[\"date\"] >= cutoff_date]\n",
        "\n",
        "# Features to use\n",
        "features = calendar_cols_only + [\"prev_day_return\"]\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[\"went_up\"]\n",
        "X_test  = test_df[features]\n",
        "y_test  = test_df[\"went_up\"]\n",
        "\n",
        "# Train new model\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict + evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Out-of-Sample Accuracy (Nov 2024–May 2025): {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605582
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 19: Advanced non-leaking price features\n",
        "\n",
        "# Step 1: Get previous day OHLC bars (from 16:00 timestamp)\n",
        "df_prev_day = full_df[(full_df[\"hour\"] == 16) & (full_df[\"minute\"] == 0)].copy()\n",
        "df_prev_day[\"date_for_merge\"] = df_prev_day[\"timestamp\"].dt.normalize() + pd.Timedelta(days=1)\n",
        "\n",
        "# Step 2: Get high/low/open/close for each previous day\n",
        "grouped = full_df.copy()\n",
        "grouped[\"date\"] = grouped[\"timestamp\"].dt.date\n",
        "day_summary = grouped.groupby(\"date\").agg({\n",
        "    \"open\": \"first\",\n",
        "    \"high\": \"max\",\n",
        "    \"low\": \"min\",\n",
        "    \"close\": \"last\"\n",
        "}).reset_index()\n",
        "day_summary[\"merge_key\"] = pd.to_datetime(day_summary[\"date\"]) + pd.Timedelta(days=1)\n",
        "\n",
        "# Step 3: Merge all into model_df_clean\n",
        "model_df_ext = model_df_clean.copy()\n",
        "model_df_ext[\"merge_key\"] = pd.to_datetime(model_df_ext[\"date\"]).dt.normalize()\n",
        "\n",
        "# Merge previous day close and open\n",
        "model_df_ext = pd.merge(model_df_ext, df_prev_day[[\"date_for_merge\", \"open\", \"close\"]], left_on=\"merge_key\", right_on=\"date_for_merge\", how=\"left\")\n",
        "model_df_ext = model_df_ext.rename(columns={\"open\": \"prev_day_open\", \"close\": \"prev_day_close\"})\n",
        "\n",
        "# Merge volatility info\n",
        "model_df_ext = pd.merge(model_df_ext, day_summary[[\"merge_key\", \"high\", \"low\", \"open\"]], on=\"merge_key\", how=\"left\")\n",
        "model_df_ext = model_df_ext.rename(columns={\"open\": \"prev_day_first_open\"})\n",
        "\n",
        "# Step 4: Create features\n",
        "model_df_ext[\"prev_day_change\"] = (model_df_ext[\"prev_day_close\"] - model_df_ext[\"prev_day_open\"]) / model_df_ext[\"prev_day_open\"]\n",
        "model_df_ext[\"overnight_gap\"] = (model_df_ext[\"close_930\"] - model_df_ext[\"prev_day_close\"]) / model_df_ext[\"prev_day_close\"]\n",
        "model_df_ext[\"prior_volatility\"] = model_df_ext[\"high\"] - model_df_ext[\"low\"]\n",
        "model_df_ext[\"prev_day_range_pct\"] = (model_df_ext[\"high\"] - model_df_ext[\"low\"]) / model_df_ext[\"prev_day_first_open\"]\n",
        "\n",
        "# Step 5: Clean\n",
        "model_df_ext = model_df_ext.dropna()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605595
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Block 20: Out-of-sample test using advanced price features\n",
        "\n",
        "# Set cutoff date for real-world backtest\n",
        "cutoff_date = pd.to_datetime(\"2024-11-01\")\n",
        "\n",
        "# Feature set: calendar + new price features\n",
        "features = calendar_cols_only + [\n",
        "    \"prev_day_change\", \"overnight_gap\", \"prior_volatility\", \"prev_day_range_pct\"\n",
        "]\n",
        "\n",
        "# Time-based split\n",
        "train_df = model_df_ext[model_df_ext[\"date\"] < cutoff_date]\n",
        "test_df  = model_df_ext[model_df_ext[\"date\"] >= cutoff_date]\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[\"went_up\"]\n",
        "X_test  = test_df[features]\n",
        "y_test  = test_df[\"went_up\"]\n",
        "\n",
        "# Train model\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict + evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Out-of-Sample Accuracy (with advanced features): {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605620
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Block 21: Probabilistic forecast with confidence threshold\n",
        "\n",
        "# Predict probabilities on test set\n",
        "probs = model.predict_proba(X_test)\n",
        "\n",
        "# Class 1 (went_up) probabilities\n",
        "confidences = probs[:, 1]\n",
        "\n",
        "# Set confidence threshold\n",
        "threshold = 0.60\n",
        "\n",
        "# Filter predictions where model is confident enough\n",
        "confident_indices = (confidences >= threshold) | (confidences <= (1 - threshold))\n",
        "confident_preds = (confidences[confident_indices] >= threshold).astype(int)\n",
        "confident_truths = y_test.iloc[confident_indices]\n",
        "\n",
        "# Evaluate accuracy on confident predictions\n",
        "confident_accuracy = accuracy_score(confident_truths, confident_preds)\n",
        "\n",
        "# Summary\n",
        "print(f\"Confidence threshold: {threshold}\")\n",
        "print(f\"Number of confident predictions: {len(confident_preds)} / {len(y_test)}\")\n",
        "print(f\"Accuracy on confident predictions: {confident_accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605634
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 22: Daily Simulation Loop\n",
        "\n",
        "# Set up\n",
        "cutoff_date = pd.to_datetime(\"2024-11-01\")\n",
        "threshold = 0.60\n",
        "\n",
        "# Create results list\n",
        "simulation_results = []\n",
        "\n",
        "# Loop through each row in out-of-sample test set\n",
        "for i, row in model_df_ext[model_df_ext[\"date\"] >= cutoff_date].iterrows():\n",
        "    features = row[calendar_cols_only + [\n",
        "        \"prev_day_change\", \"overnight_gap\", \"prior_volatility\", \"prev_day_range_pct\"\n",
        "    ]].values.reshape(1, -1)\n",
        "\n",
        "    # Predict probability\n",
        "    prob = model.predict_proba(features)[0][1]  # Probability SPY will go up\n",
        "\n",
        "    # Decide action\n",
        "    if prob >= threshold:\n",
        "        prediction = 1\n",
        "        acted = True\n",
        "    elif prob <= (1 - threshold):\n",
        "        prediction = 0\n",
        "        acted = True\n",
        "    else:\n",
        "        prediction = -1  # model skipped\n",
        "        acted = False\n",
        "\n",
        "    # Store result\n",
        "    simulation_results.append({\n",
        "        \"date\": row[\"date\"],\n",
        "        \"prob_up\": prob,\n",
        "        \"prediction\": prediction,\n",
        "        \"actual\": row[\"went_up\"],\n",
        "        \"acted\": acted,\n",
        "        \"correct\": (prediction == row[\"went_up\"]) if acted else None\n",
        "    })\n",
        "\n",
        "# Turn into DataFrame\n",
        "simulation_df = pd.DataFrame(simulation_results)\n",
        "\n",
        "# Print simulation summary\n",
        "num_acted = simulation_df[\"acted\"].sum()\n",
        "accuracy = simulation_df[simulation_df[\"acted\"] == True][\"correct\"].mean()\n",
        "\n",
        "print(f\"Confidence threshold: {threshold}\")\n",
        "print(f\"Decisions made: {num_acted} / {len(simulation_df)} days\")\n",
        "print(f\"Accuracy on decisions: {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605646
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 23: High-confidence simulation (threshold ≥ 0.70)\n",
        "\n",
        "threshold = 0.70  # NEW threshold for strong signals\n",
        "\n",
        "# Re-run the loop using new threshold\n",
        "high_conf_results = []\n",
        "\n",
        "for i, row in model_df_ext[model_df_ext[\"date\"] >= cutoff_date].iterrows():\n",
        "    features = row[calendar_cols_only + [\n",
        "        \"prev_day_change\", \"overnight_gap\", \"prior_volatility\", \"prev_day_range_pct\"\n",
        "    ]].values.reshape(1, -1)\n",
        "\n",
        "    # Predict probability\n",
        "    prob = model.predict_proba(features)[0][1]\n",
        "\n",
        "    # Decide action\n",
        "    if prob >= threshold:\n",
        "        prediction = 1\n",
        "        acted = True\n",
        "    elif prob <= (1 - threshold):\n",
        "        prediction = 0\n",
        "        acted = True\n",
        "    else:\n",
        "        prediction = -1\n",
        "        acted = False\n",
        "\n",
        "    # Store result\n",
        "    high_conf_results.append({\n",
        "        \"date\": row[\"date\"],\n",
        "        \"prob_up\": prob,\n",
        "        \"prediction\": prediction,\n",
        "        \"actual\": row[\"went_up\"],\n",
        "        \"acted\": acted,\n",
        "        \"correct\": (prediction == row[\"went_up\"]) if acted else None\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "high_conf_df = pd.DataFrame(high_conf_results)\n",
        "\n",
        "# Report\n",
        "num_acted = high_conf_df[\"acted\"].sum()\n",
        "accuracy = high_conf_df[high_conf_df[\"acted\"] == True][\"correct\"].mean()\n",
        "\n",
        "print(f\"High-confidence threshold: {threshold}\")\n",
        "print(f\"Decisions made: {num_acted} / {len(high_conf_df)} days\")\n",
        "print(f\"Accuracy on confident predictions: {accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605666
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Block 24: Add technical indicators (RSI and SMA)\n",
        "df_tech = full_df.copy()\n",
        "df_tech[\"date\"] = df_tech[\"timestamp\"].dt.date\n",
        "\n",
        "# Step 1: Group daily close for RSI/SMA\n",
        "daily_closes = df_tech.groupby(\"date\")[\"close\"].last().reset_index()\n",
        "daily_closes[\"date\"] = pd.to_datetime(daily_closes[\"date\"])\n",
        "daily_closes = daily_closes.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# Step 2: Calculate SMA features\n",
        "daily_closes[\"sma_5\"] = daily_closes[\"close\"].rolling(window=5).mean()\n",
        "daily_closes[\"sma_20\"] = daily_closes[\"close\"].rolling(window=20).mean()\n",
        "daily_closes[\"sma_ratio\"] = daily_closes[\"sma_5\"] / daily_closes[\"sma_20\"]\n",
        "daily_closes[\"sma_distance\"] = (daily_closes[\"close\"] - daily_closes[\"sma_20\"]) / daily_closes[\"sma_20\"]\n",
        "\n",
        "# Step 3: Calculate RSI (14-period)\n",
        "delta = daily_closes[\"close\"].diff()\n",
        "gain = np.where(delta > 0, delta, 0)\n",
        "loss = np.where(delta < 0, -delta, 0)\n",
        "avg_gain = pd.Series(gain).rolling(window=14).mean()\n",
        "avg_loss = pd.Series(loss).rolling(window=14).mean()\n",
        "rs = avg_gain / avg_loss\n",
        "daily_closes[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "# Step 4: Merge into model_df_ext\n",
        "model_df_ext[\"date\"] = pd.to_datetime(model_df_ext[\"date\"])\n",
        "model_df_ext = pd.merge(model_df_ext, daily_closes[[\"date\", \"sma_5\", \"sma_20\", \"sma_ratio\", \"sma_distance\", \"rsi_14\"]], on=\"date\", how=\"left\")\n",
        "\n",
        "# Step 5: Drop any rows with missing values\n",
        "model_df_ext = model_df_ext.dropna(subset=[\"sma_5\", \"sma_20\", \"rsi_14\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605686
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 25: Retrain with technical indicators\n",
        "\n",
        "# Feature list\n",
        "tech_features = [\n",
        "    \"prev_day_change\", \"overnight_gap\", \"prior_volatility\", \"prev_day_range_pct\",\n",
        "    \"sma_ratio\", \"sma_distance\", \"rsi_14\"\n",
        "]\n",
        "all_features = calendar_cols_only + tech_features\n",
        "\n",
        "# Split again\n",
        "cutoff_date = pd.to_datetime(\"2024-11-01\")\n",
        "train_df = model_df_ext[model_df_ext[\"date\"] < cutoff_date]\n",
        "test_df  = model_df_ext[model_df_ext[\"date\"] >= cutoff_date]\n",
        "\n",
        "X_train = train_df[all_features]\n",
        "y_train = train_df[\"went_up\"]\n",
        "X_test  = test_df[all_features]\n",
        "y_test  = test_df[\"went_up\"]\n",
        "\n",
        "# Retrain model\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "threshold = 0.70\n",
        "\n",
        "# Filter by confidence\n",
        "confident_indices = (probs >= threshold) | (probs <= (1 - threshold))\n",
        "confident_preds = (probs[confident_indices] >= threshold).astype(int)\n",
        "confident_truths = y_test.iloc[confident_indices]\n",
        "\n",
        "# Results\n",
        "confident_accuracy = accuracy_score(confident_truths, confident_preds)\n",
        "print(f\"Confidence threshold: {threshold}\")\n",
        "print(f\"Decisions made: {len(confident_preds)} / {len(y_test)}\")\n",
        "print(f\"Accuracy on confident predictions: {confident_accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605709
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Block 26: Simulate trading P&L from high-confidence predictions\n",
        "\n",
        "# Filter confident predictions (already done)\n",
        "simulation_df = test_df.copy()\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "simulation_df[\"prob_up\"] = probs\n",
        "simulation_df[\"acted\"] = (probs >= 0.70) | (probs <= 0.30)\n",
        "simulation_df[\"prediction\"] = np.where(probs >= 0.70, 1, np.where(probs <= 0.30, 0, -1))\n",
        "simulation_df[\"correct\"] = simulation_df[\"prediction\"] == simulation_df[\"went_up\"]\n",
        "simulation_df[\"pnl\"] = np.where(simulation_df[\"acted\"], np.where(simulation_df[\"correct\"], 1, -1), 0)\n",
        "simulation_df[\"cum_pnl\"] = simulation_df[\"pnl\"].cumsum()\n",
        "\n",
        "# Plot cumulative return\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(simulation_df[\"date\"], simulation_df[\"cum_pnl\"], label=\"Cumulative Return\", color=\"green\")\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "plt.title(\"Simulated Cumulative Return (Confidence ≥ 70%)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative Units\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "total_return = simulation_df[\"cum_pnl\"].iloc[-1]\n",
        "num_trades = simulation_df[\"acted\"].sum()\n",
        "win_rate = simulation_df[simulation_df[\"acted\"]][\"correct\"].mean()\n",
        "\n",
        "print(f\"Trades made: {num_trades}\")\n",
        "print(f\"Final cumulative return: {total_return}\")\n",
        "print(f\"Win rate on trades: {win_rate:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605727
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_features = calendar_cols_only + [\n",
        "    \"prev_day_change\", \"overnight_gap\", \"prior_volatility\", \"prev_day_range_pct\",\n",
        "    \"sma_ratio\", \"sma_distance\", \"rsi_14\"\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605748
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot top 15 most important features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_importance(model, max_num_features=15, importance_type='gain')\n",
        "plt.title(\"XGBoost Feature Importance (Gain)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605757
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print raw feature importances as a dictionary\n",
        "import pandas as pd\n",
        "\n",
        "importance_dict = model.get_booster().get_score(importance_type='gain')\n",
        "importance_df = pd.DataFrame(\n",
        "    sorted(importance_dict.items(), key=lambda x: x[1], reverse=True),\n",
        "    columns=[\"Feature\", \"Importance\"]\n",
        ")\n",
        "\n",
        "print(importance_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605767
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 30: Retrain with refined features based on importance\n",
        "\n",
        "# Step 1: Define refined feature set\n",
        "refined_features = [\n",
        "    \"is_quarter_end\", \"is_month_start\", \"is_month_end\",\n",
        "    \"macd_diff\", \"volume_5day_ratio\", \"prev_day_change\",\n",
        "    \"bollinger_width\", \"prior_volatility\", \"overnight_gap\",\n",
        "    \"rsi_14\", \"sma_ratio\", \"bollinger_position\"\n",
        "]\n",
        "\n",
        "# Step 2: Create new train/test sets\n",
        "cutoff_date = pd.to_datetime(\"2024-11-01\")\n",
        "train_df = model_df_ext[model_df_ext[\"date\"] < cutoff_date]\n",
        "test_df  = model_df_ext[model_df_ext[\"date\"] >= cutoff_date]\n",
        "\n",
        "X_train = train_df[refined_features]\n",
        "y_train = train_df[\"went_up\"]\n",
        "X_test  = test_df[refined_features]\n",
        "y_test  = test_df[\"went_up\"]\n",
        "\n",
        "# Step 3: Retrain model\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "threshold = 0.70\n",
        "\n",
        "# Step 5: Simulate confident predictions\n",
        "confident_indices = (probs >= threshold) | (probs <= (1 - threshold))\n",
        "confident_preds = (probs[confident_indices] >= threshold).astype(int)\n",
        "confident_truths = y_test.iloc[confident_indices]\n",
        "\n",
        "# Step 6: Evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "confident_accuracy = accuracy_score(confident_truths, confident_preds)\n",
        "print(f\"Confidence threshold: {threshold}\")\n",
        "print(f\"Decisions made: {len(confident_preds)} / {len(y_test)}\")\n",
        "print(f\"Accuracy on confident predictions: {confident_accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605778
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 31: Sweep confidence thresholds (70%, 75%, 80%)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predict probabilities\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "thresholds = [0.70, 0.75, 0.80]\n",
        "\n",
        "# Evaluate each threshold\n",
        "for threshold in thresholds:\n",
        "    confident_indices = (probs >= threshold) | (probs <= (1 - threshold))\n",
        "    confident_preds = (probs[confident_indices] >= threshold).astype(int)\n",
        "    confident_truths = y_test.iloc[confident_indices]\n",
        "    confident_accuracy = accuracy_score(confident_truths, confident_preds)\n",
        "\n",
        "    print(f\"\\nThreshold: {threshold:.2f}\")\n",
        "    print(f\"Decisions made: {len(confident_preds)} / {len(y_test)}\")\n",
        "    print(f\"Accuracy on confident predictions: {confident_accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605796
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 32: Trade only on high volume + high confidence days\n",
        "\n",
        "threshold = 0.70  # keep best-performing threshold\n",
        "\n",
        "# Predict probabilities\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Filter by confidence\n",
        "conf_mask = (probs >= threshold) | (probs <= (1 - threshold))\n",
        "volume_mask = test_df[\"volume_5day_ratio\"] > 1.00  # trade only if volume above 5-day avg\n",
        "\n",
        "# Combine masks\n",
        "final_mask = conf_mask & volume_mask\n",
        "\n",
        "# Predictions + truth (FIXED)\n",
        "confident_preds = (probs[final_mask] >= threshold).astype(int)\n",
        "confident_truths = y_test.loc[final_mask]\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "confident_accuracy = accuracy_score(confident_truths, confident_preds)\n",
        "print(f\"Filtered by: confidence ≥ {threshold} AND volume_5day_ratio > 1\")\n",
        "print(f\"Decisions made: {len(confident_preds)} / {len(y_test)}\")\n",
        "print(f\"Accuracy on confident predictions: {confident_accuracy:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605811
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Block 33: Simulated return for filtered model\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "filtered_df = test_df.copy()\n",
        "filtered_df[\"prob_up\"] = probs\n",
        "filtered_df[\"acted\"] = ((probs >= 0.70) | (probs <= 0.30)) & (filtered_df[\"volume_5day_ratio\"] > 1.0)\n",
        "filtered_df[\"prediction\"] = np.where(probs >= 0.70, 1, np.where(probs <= 0.30, 0, -1))\n",
        "filtered_df[\"correct\"] = filtered_df[\"prediction\"] == filtered_df[\"went_up\"]\n",
        "filtered_df[\"pnl\"] = np.where(filtered_df[\"acted\"], np.where(filtered_df[\"correct\"], 1, -1), 0)\n",
        "filtered_df[\"cum_pnl\"] = filtered_df[\"pnl\"].cumsum()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(filtered_df[\"date\"], filtered_df[\"cum_pnl\"], label=\"Cumulative Return\", color=\"blue\")\n",
        "plt.axhline(0, color=\"gray\", linestyle=\"--\")\n",
        "plt.title(\"Simulated Cumulative Return\\n(Confidence ≥ 70% + Volume > 5-Day Avg)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative Units\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "total_return = filtered_df[\"cum_pnl\"].iloc[-1]\n",
        "num_trades = filtered_df[\"acted\"].sum()\n",
        "win_rate = filtered_df[filtered_df[\"acted\"]][\"correct\"].mean()\n",
        "\n",
        "print(f\"Trades made: {num_trades}\")\n",
        "print(f\"Final cumulative return: {total_return}\")\n",
        "print(f\"Win rate on trades: {win_rate:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605823
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_df_fixed.columns.tolist())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605843
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1 (Revised): Add return_overnight and previous day range %\n",
        "\n",
        "# Return from previous day's close to today's open (gap)\n",
        "model_df_fixed[\"return_overnight\"] = (\n",
        "    (model_df_fixed[\"close_930\"] - model_df_fixed[\"prev_day_close\"]) / model_df_fixed[\"prev_day_close\"]\n",
        ")\n",
        "\n",
        "# Previous day range percent (if you already have it, this is just reconfirming)\n",
        "model_df_fixed[\"prev_day_range_pct\"] = (\n",
        "    abs(model_df_fixed[\"prev_day_close\"] - model_df_fixed[\"prev_day_open\"]) / model_df_fixed[\"prev_day_open\"]\n",
        ")\n",
        "\n",
        "# Clean up any NaNs\n",
        "model_df_fixed = model_df_fixed.dropna(subset=[\"return_overnight\", \"prev_day_range_pct\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605853
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reattach technical indicators to model_df_fixed\n",
        "tech_cols = [\n",
        "    \"macd_diff\", \"volume_5day_ratio\", \"bollinger_width\", \"bollinger_position\",\n",
        "    \"prior_volatility\", \"overnight_gap\", \"rsi_14\", \"sma_ratio\"\n",
        "]\n",
        "\n",
        "model_df_fixed = pd.merge(\n",
        "    model_df_fixed,\n",
        "    model_df_ext[[\"date\"] + tech_cols],\n",
        "    on=\"date\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Drop any rows where these new features are still missing\n",
        "model_df_fixed = model_df_fixed.dropna(subset=tech_cols)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605863
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Retrain with new engineered features\n",
        "\n",
        "# Updated refined feature set\n",
        "refined_features_v2 = [\n",
        "    \"is_quarter_end\", \"is_month_start\", \"is_month_end\",\n",
        "    \"macd_diff\", \"volume_5day_ratio\", \"prev_day_change\",\n",
        "    \"bollinger_width\", \"prior_volatility\", \"overnight_gap\",\n",
        "    \"rsi_14\", \"sma_ratio\", \"bollinger_position\",\n",
        "    \"return_overnight\", \"prev_day_range_pct\"\n",
        "]\n",
        "\n",
        "# Train/test split\n",
        "cutoff_date = pd.to_datetime(\"2024-11-01\")\n",
        "train_df = model_df_fixed[model_df_fixed[\"date\"] < cutoff_date]\n",
        "test_df  = model_df_fixed[model_df_fixed[\"date\"] >= cutoff_date]\n",
        "\n",
        "X_train = train_df[refined_features_v2]\n",
        "y_train = train_df[\"went_up\"]\n",
        "X_test  = test_df[refined_features_v2]\n",
        "y_test  = test_df[\"went_up\"]\n",
        "\n",
        "# Retrain model\n",
        "model = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Confidence filter: 70%\n",
        "threshold = 0.70\n",
        "conf_mask = (probs >= threshold) | (probs <= (1 - threshold))\n",
        "vol_mask = test_df[\"volume_5day_ratio\"] > 1.0\n",
        "final_mask = conf_mask & vol_mask\n",
        "\n",
        "# Final evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "preds = (probs[final_mask] >= threshold).astype(int)\n",
        "truth = y_test.loc[final_mask]\n",
        "acc = accuracy_score(truth, preds)\n",
        "\n",
        "print(f\"Final refined model with new features\")\n",
        "print(f\"Confidence threshold: {threshold}\")\n",
        "print(f\"Volume filter: > 5-day avg\")\n",
        "print(f\"Decisions made: {len(preds)} / {len(y_test)}\")\n",
        "print(f\"Accuracy on confident predictions: {acc:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1746780605874
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}